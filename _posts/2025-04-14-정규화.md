---
layout: article
title: "[DL Basic] 정규화(Regularization)"
date: 2025-04-14 13:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---

## 정규화(Regularization, 규제화)

- 이전에 신경망을 학습 할 때, 좋은 위치에서 시작하는 가중치 초기화에 대하여 알아보았다. 이와 더불어 최적해로 가는 길을 잘 찾을 수 있도록 해주는 정규화에 대해 알아보려 한다.
- **정규화란 최적화 과정에서 최적해를 잘 찾도록 정보를 추가하는 기법**으로, 성능을 개선할 수 있는 포괄적인 기법들을 포함한다.
- 여기서 **성능이 좋다는 것은 일반화가 잘 되었다**는 것으로 **훈련 성능과 검증/테스트 성능의 차이를 나타내는 일반화 오류**가 적다는 것이다.

### 정규화의 접근 방식

1. **모델을 최대한 단순하게 만들기**
    - 복잡한 모델의 파라미터 수를 줄이거나, 파라미터를 제거하는 등의 기법으로 **모델을 단순하게 만들어 과적합을 줄이는 것**이다.
2. **사전 지식을 표현해서 최적해를 빠르게 찾도록 함**
    - 데이터나 모델에 대한 사전 분포를 이용하여 정확하고 빠르게 해를 찾는 방법으로 가중치 감소가 그 예이다.
3. **확률적 성질을 추가**
    - 데이터, 모델, 훈련 기법 등에 확률적 성질을 추가하여 조금씩 변형된 형태로 데이터를 다루어 다양한 상황에서 학습하는 효과를 준다.
    - 이는 손실 함수가 풍부한 데이터를 이용하여 넓은 범위를 세밀하게 표현하도록 하여 더 정확한 해를 찾고, Noise에 민감하지 않다.
    - 데이터 증강, 잡음 주입, 드롭 아웃 등의 방식을 이용한다.
4. **여러 가설을 고려하여 예측**
    - 앙상블 같은 방법을 이용하여 하나의 모델이 가지는 편향을 제거하여 오차를 최소화하는 방법이다.

---

### 배치 정규화

- 신경망의 학습이 어려운 이유 중 하나는 **계층을 지날 때마다 데이터의 분포가 보이지 않는 요인에 의해 왜곡**되기 때문이다. 이런 현상을 **내부 공변량 변화**라고 한다. 그리고 **분포를 결정하는 보이지 않는 요인을 내부 공변량**이라 한다.
- 이 내부 공변량이 계층을 지나면서 바뀌어서 각 계층에서 데이터의 분포가 원래 분포에서 조금씩 멀어지게 되고, 이는 계층을 지날 때마다 심해진다.

![스크린샷 2025-04-15 175848](https://github.com/user-attachments/assets/e25152da-b552-416b-a942-2867553466d4)

- 기존의 내부 공변량 변화를 막기 위해서 가중치 초기화를 잘하고, 학습률도 작게 사용해야 했다. 하지만 이 경우에는 학습 속도가 느린 문제가 있다.
- 이 내부 공변량 변화를 해결하기 위한 것이 배치 정규화이다. 배치 정규화는 **계층을 지날 때마다 매번 정규화**를 하여 내부 공변량 변화를 없앤다.(주로 뉴런의 가중 합산과 활성화 함수 사이에서 진행)
- 기존의 정규화와 다른 점은 **모델의 계층 형태로 데이터 정규화를 한다**는 점과 **전체 데이터가 아닌 미니 배치에 대해 정규화**한다는 점이다.
- 배치 정규화의 과정은 우선 **표준 가우시안 분포로 정규화**를 한 뒤, 학습 파라미터인 **$\gamma$와 $\beta$를 통해 원래 분포로 복원**하면 된다.
- 표준 가우시안 분포로 정규화를 하여 데이터가 계층을 지날 때마다 표준 가우시안 분포로 바뀌어 내부 공변량 변화를 최소화 하게 된다.
- 하지만, **표준 가우시안 분포는 모델이 비선형성을 제대로 표현할 수 없고**, 우리가 원하는 것은 표준화가 아닌 **입출력의 분포를 맞추는 것**이기에 원래 분포로 복구해주는 작업을 거친다.
- **표준 가우시안이 비선형성을 제대로 표현 못하는 이유**
    - 우선 정규화 이후에 활성화 함수를 거치는데 활성화 함수가 Sigmoid인 경우부터 보자
    - 정규화를 통해 데이터의 범위는 [-1, 1]이 될 것이다. 하지만 Sigmoid에서 해당 범위는 선형이기에 비선형성이 사라진다.
    - ReLU에 적용해보면 [-1, 1] 범위에서 절반은 0이 되어 학습이 어렵고, 나머지 절반도 선형적이므로 비선형성이 줄어든다.
    
    ![스크린샷 2025-04-15 175908](https://github.com/user-attachments/assets/cd74c792-33fc-4c66-9297-804d555be13e)

- 배치 정규화를 통해 **내부 공변량 변화가 억제되면**, 각 계층의 입력 분포가 안정되기 때문에 **그레디언트 흐름이 안정화되고 학습 속도와 성능이 향상**된다.
- 또한 데이터의 분포를 유지하려 하므로, **가중치 초기화의 의존도가 낮아지고 높은 학습률을 사용**할 수 있다.
- 추가적으로 미니 배치 별로 정규화하여 미니 배치의 조합에 따라 데이터가 조금씩 변형되어 **확률적 성질까지 부여**하여 성능이 올라간다.

---

### 가중치 감소

- 선형 회귀를 가정했을 때, $w^{T}x + b = 0$과 2를 곱한 $2w^{T}x+2b$는 완전히 같은 직선이다. 이렇듯 하나의 직선을 표현할 수 있는 표현은 굉장히 많다.
- 이 중에서 가장 좋은 식은 $w^{T}x+b=0$이다. 왜냐하면 **최적화 시에 다루는 숫자의 크기가 작을 수록 오차의 변동성이 낮아지기 때문**이다.(파라미터 값이 작을수록 손실 함수의 변화가 더 매끄럽고 안정적)
- **즉, 파라미터 공간이 원점 근처에 있을 때, 정확한 해를 빠르게 찾을 수 있는 것이다.** 그렇기에 학습 시에 weight와 bias는 작은 것이 좋다.
- **학습 과정에서 작은 크기의 가중치를 갖게 해주는 것이 가중치 감소**이다.
- 가중치 감소는 $J(w) = J(w) + \lambda R(w)$와 같이 손실 함수($J(W)$)에 가중치의 크기를 표현하는 정규화 항($\lambda R(W)$)을 더하여 사용한다. $\lambda$는 정규화 상수로서, 가중치의 크기를 조절해주는 역할을 한다.($\lambda$가 크면 정규화 항의 비중이 커지며, 가중치의 크기가 더 작아진다.)
    
    
    |  | $\lambda$  커짐 | $\lambda$ 작아짐 |
    | --- | --- | --- |
    | 정규항의 비중 | 비중이 커짐 | 비중이 작아짐 |
    | 가중치의 크기 | 더 작아짐 | 더 커짐 |

- 주로 정규화 항은  $L_{2}$노름이나 $L_{1}$노름을 사용한다. **$L_{2}$노름을 사용하면 리지 회귀**, **$L_{1}$노름을 사용하면 라소 회귀**라고 한다.

![스크린샷 2025-04-15 201116](https://github.com/user-attachments/assets/d3b16e9c-3790-4920-b6a2-f2f621d3e899)

- **리지 회귀는 주로 가중치의 사전 분포를 모르거나, 가우시안 분포일 경우 사용**하고, **라소 회귀는 데이터의 사전 분포가 라플라스 분포일 경우에 사용**한다.
- 또한, 가중치 감소는 정규화의 효과를 볼 수 있다. 원래 손실 함수 값은 최적해 방향으로 가려하지만, 정규화 항은 다른 방향으로 가려고 하기에 최적해 부분이 아닌 정규화 항과 손실 함수의 접점이 해가 된다.
- 전역 최적해는 과적합 일수도 있기에, 거기에서 떨어진 부분의 가중치를 사용하여 과적합을 방지하기에 정규화의 효과를 볼 수 있는 것이다.

![스크린샷 2025-04-15 201714](https://github.com/user-attachments/assets/42878b94-c772-42a1-853b-b4fa84c36200)

- 위의 그림을 보면 리지 회귀를 보면, 손실 함수가 어느 곳에 있던지 정규화 항이 원 모양이기에 접할 확률은 손실 함수가 어디에 있던지 동일하다.
- 하지만 라소 회귀의 경우, 마름모 모양으로 꼭짓점 부분에서 접할 확률이 가장 높다. 이는 최적해가 특정 축 위에 있을 확률이 높다는 것이고, 나머지 축의 값은 0이 된다는 것이다. → 정규화 접근 방식의 “모델을 최대한 단순하게 만들기**”**에 해당되고, 리지 회귀는 정규화 접근 방식의 “사전 지식을 표현해서 최적해를 빠르게 찾도록한다.”에 해당한다.

---

### 조기 종료

- **조기 종료란 모델이 과적합되기 전에 훈련을 멈추는 정규화 기법**이다.
- 훈련 성능과 테스트/검증 성능을 비교하여, 이 둘의 차이가 벌어지는 부분 즉, **훈련 성능은 계속 향상하지만, 테스트/검증 성능은 향상 하지 않는 경우에 멈추는 것**이다.(여기에서 성능은 주로 Loss나 평가 지표를 기준)
- 이 때, 주의할 점은 **모델의 성능이 향상하지 않더라도 바로 종료하는 것이 아니라, 몇 번 더 지켜본 뒤에 멈추는 것**이다. 이것은 미니 배치로 근사한 그레디언트와 실제 그레디언트는 차이가 있기에 성능이 좋았다 나빴다 할 수 있기 때문이다.
- 즉, 일시적인 성능 변동이 아닌 **지속적인 성능의 정체나 하락이 보일 때, 종료하는 것**이 좋다.
- 조기 종료는 파라미터 공간을 작게 만들어, 크기를 제약시키므로 $L_{2}$**정규화와 동일한 효과**를 볼 수 있다.
    
    ![스크린샷 2025-04-15 204031](https://github.com/user-attachments/assets/d7602e15-c9ef-492c-91d5-20b465e30414)


---

### 데이터 증강

- 모델의 복잡도에 비해 충분한 훈련 데이터가 제공되지 않으면 모델이 데이터를 암기해서 과적합이 생기게 된다.
- 이 과적합을 막는 가장 근본적인 방법은 훈련 데이터의 양을 늘리는 것이다. **훈련 데이터의 양이 크면 훈련 데이터의 오류는 약간 증가하지만, 테스트 오류는 감소하여, 일반화 오류 또는 과적합 정도가 줄어들게 된다.**
    
    ![스크린샷 2025-04-15 212600](https://github.com/user-attachments/assets/ebb84234-6ab2-42e2-acd7-298e8c3230b0)

- 하지만 데이터셋을 증가시키기 위해, 데이터를 더 수집하는 것은 어렵기에 **기존의 가지고 있는 훈련 데이터 셋을 이용하여 새로운 데이터를 만드는 것이 데이터 증강**이다.
- 이렇게 증강한 데이터를 미리 훈련 데이터 셋에 추가하여 사용할 수도 있지만, **일반적으로는 훈련 과정에서 실시간으로 데이터를 증강한다.** 이는 데이터를 확률적으로 변형하기에 무한히 많은 변형이 생길 수 있기 때문이다.
- 가장 기본적인 데이터 증강으로는 **훈련 데이터를 조금씩 변형해서 새로운 데이터를 만드는 방법**이고, 여기서 좀 더 나아가면, **학습 데이터로 학습한 생성모델로 새로운 데이터를 생성하는 방법**이 있다.
- 증강 방법은 굉장히 다양한데 이때는 데이터의 종류와 문제, 하고자 하는 Task에 따라 고려하여 선택해야 한다.
- 데이터 증강 시 주의할 점으로는 **클래스 불변 가정**을 따라야 한다. 이는 클래스의 결정 경계 안에서 데이터를 변형해야 하는 것으로 **증강할 때, 클래스가 바뀌지 않도록 해야한다는 것**이다.

---

### 배깅

- 배깅은 여러 모델을 실행하여 하나의 강한 모델을 만드는 방법인 앙상블 기법 중 하나로 **독립된 여러 모델을 동시에 실행**한 뒤 개별 모델의 예측을 이용하여 최종으로 예측하는 방법이다.
- 배깅은 모델의 종류와 관계 없이 다양한 모델로 팀을 구성할 수 있다. 단, 성능을 높이려면 **모델 간의 독립을 보장**해야한다.
- 배깅에서는 모델 간의 독립을 더욱 보장하기 위해 **모델 별로 부트스트랩 데이터를 사용**한다. 부트스트랩이란 훈련 데이터에서 복원 추출하여 훈련 데이터와 같은 크기로 만드는 것이다.
- 하지만 신경망 모델에서는 부트스트랩 안해도 된다.  **랜덤한 값으로 가중치를 초기화** 하고, **미니 배치를 사용하기에 모델 별로 다른 데이터 셋을 사용하는 것 같은 효과**를 주기 때문이다.
- 이 후, 추론 단계에서는 **개별 모델의 결과를 집계하여 예측**한다. 예를 들어, 회귀 문제에서는 예측 값들을 평균을 낸다거나, 분류 문제에서는 Voting을 사용하여 추론한다.
- 배깅의 정규화 효과를 확인하기 위해 개별 모델의 예측 오차가 배깅에서 어떻게 줄어드는지 보자.
    
    ![스크린샷 2025-04-15 214122](https://github.com/user-attachments/assets/dead2341-182a-4c53-ac41-02e71f4ad926)

- 여기에서 $\epsilon_{i}$는 개별 모델의 오차이고, 개별 모델의 오차는 평균이 0이고, 분산이 $v$이고, 모델 간의 공분산을 $c$로 표현한 것이다. 위 그림에서는 배깅에 의해 오차가 줄었는지 확인하기 위해 오차의 분산을 구한 것이다.
- 우선 개별 모델이 독립일 때를 가정하자. 그럼 공분산($c$)가 0이 될 것이다. 그럼 오차의 분산은 $\frac v{k}$가 되어 **모델의 개수(k)에 비례하여 분산이 감소하게 된다.**
- 이와 반대로 개별 모델 간의 상관성이 커서 공분산($c$)와 $v$가 같다고 가정하면 오차의 분산은 $v$가 되어 오차의 분산이 개별 모델과 동일해진다.
- **즉, 배깅으로 효과를 보려면, 개별 모델들이 독립이여야한다.**

---

### 드롭아웃

- **드롭 아웃은 미니 배치를 실행할 때마다 뉴련을 랜덤하게 잘라 새로운 모델을 생성하는 정규화 기법**이다. 이 때, 각 계층마다 다른 드롭 아웃 확률을 지정할 수 있고, 각 계층의 각 뉴런들마다 지정한 확률로 살아 남는다.(주로 50% 이상으로 지정, 또한 입력과 은닉 뉴런에 지정, 입력은 더 높은 확률 부여, 출력에는 일반적으로 지정 X)
    
    ![스크린샷 2025-04-15 221018](https://github.com/user-attachments/assets/b2175bdb-c2d3-4776-93f2-5c60f780d9fb)

- 이는 확률적으로 뉴련을 지우기에 하나의 신경망 모델에서 다양한 모델을 생성하는 것으로 볼 수 있기에 배깅과 같은 효과를 볼 수 있다. 하지만 **드롭아웃은 모델 간의 독립성이 보장되지 않는 문제**가 있다.
- 그렇다면 배깅과 드롭아웃을 비교해보면, 모델 간의 독립성을 보장하는 **배깅보다는 드롭아웃은 성능이 떨어지겠지만 더 실용적**이라는 장점이 있다.
- 훈련 단계에서는 미니배치마다 베르누이 분포에서 각 계층의 뉴런별로 난수를 발생시켜 0과 1로 이루어진 이진 마스크를 생성한다. 이 이진 마스크를 해당 계층에 적용하여 마스크가 1인 부분은 살아남고, 0인 부분은 죽게 된다.
- 추론 단계에서는 뉴런을 드롭아웃하지 않고 훈련 과정에서 확률적으로 생성했던 다양한 모델의 평균을 예측한다. 모델들의 평균을 구해보면 원래 output에 드롭아웃 확률(p)의 곱으로 나타나는 것을 알 수 있다. 즉 **드롭아웃의 각 계층의 출력은 원래 출력에 드롭 아웃 확률(p)를 곱**해주면 된다.
- 위의 내용을 기반으로 했을 때, 훈련 시점에 각 계층의 출력을 미리 드롭아웃 확률(p)로 나눠주면 원래의 값을 그대로 사용할 수 있다. 이 방식이 **역 드롭아웃**이다. → $a^{(l)} = (a^{(l)} \bigodot r^{(l)}) / p$

---

### 잡음 주입

- 데이터나 모델을 확률적으로 정의할 수 있다면 더 정확하게 추론할 수 있다. 하지만 데이터나 모델이 확률적으로 정의되지 않았다면 **간단한 잡음을 넣어서 확률적 성질을 부여**할 수 있다. 이것이 **잡음 주입**이다.
- **잡음 주입 방식**
    1. **입력 데이터에 잡음 주입**
        - 입력 데이터에 잡음을 추가하는 것은 **데이터 증강 기법에 해당**하고, **아주 작은 분산을 갖는 잡음을 넣으면 가중치 감소**와 동일한 정규화 효과를 볼 수 있다.
    2. **특징에 잡음 주입**
        - **은닉 계층에서 추출된 특성에 잡음을 넣는 것으로 추상화된 상태에서 데이터 증강을 하는 것**이다.
        - 추상화된 데이터에 확률적 성질을 부여하기에 상대적으로 의미 있는 단위로 데이터 증강이 일어나 성능이 크게 향상된다.
    3. **모델 가중치에 잡음 주입**
        - 대표적인 예시가 드롭아웃이다. 드롭아웃에서 뉴런을 제거할 때 확률적으로 가중치를 조절하기에 가중치에 잡음을 넣는 것이라 볼 수 있다.
            
            ![스크린샷 2025-04-15 232705](https://github.com/user-attachments/assets/839aba0e-e271-4aa9-87a8-631560e31ef4)

        - 또한 최소 지점의 경사가 가파른 경우, 일반화 성능이 떨어질 수 있는데 **가중치에 잡음을 넣으면 최소 지점 주변이 평평해져 일반화 성능이 향상된다.**
        - 위의 그림에서 보면 **경사가 가파른 곳의 최소값은 훈련 함수와 테스트 함수가 조금만 달라도 큰 차이를 내는 것**을 볼 수 있기에 최소 지점이 평평한 곳에 있는 것이 좋다.
    4. **소프트 레이블링**
        - 분류 문제에서 훈련 데이터의 레이블에 잡음을 주는 것이다. 예를 들어, [1, 0, 0, 0]의 라벨을 [0.7, 0.1, 0.1, 0.1]로 하는 것이다.
        - 레이블에 오차가 있을 경우, 모델이 정확히 1이나 0으로 예측하지 못하기 때문에 계속하여 일정한 손실이 발생하고 최적화가 이뤄지지 않을 수 있다.
        - 이를 일정량의 오차가 있다고 가정하여, 타겟 클래스의 확률을 줄이고, 줄인 만큼을 나머지 클래스에 분배하는 방식이다.(약간의 여지를 준다고 볼 수 있음) 이렇게 레이블에 오차를 반영한 후 학습하면 모델의 성능이 높아진다.

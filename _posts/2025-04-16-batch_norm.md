---
layout: article
title: "[DL Basic] 배치 정규화(Batch Normalization)"
date: 2025-04-16 13:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---

### 배치 정규화

- 신경망의 학습이 어려운 이유 중 하나는 **계층을 지날 때마다 데이터의 분포가 보이지 않는 요인에 의해 왜곡**되기 때문이다. 이런 현상을 **내부 공변량 변화**라고 한다. 그리고 **분포를 결정하는 보이지 않는 요인을 내부 공변량**이라 한다.
- 이 내부 공변량이 계층을 지나면서 바뀌어서 각 계층에서 데이터의 분포가 원래 분포에서 조금씩 멀어지게 되고, 이는 계층을 지날 때마다 심해진다.

![스크린샷 2025-04-15 175848](https://github.com/user-attachments/assets/e25152da-b552-416b-a942-2867553466d4)

- 기존의 내부 공변량 변화를 막기 위해서 가중치 초기화를 잘하고, 학습률도 작게 사용해야 했다. 하지만 이 경우에는 학습 속도가 느린 문제가 있다.
- 이 내부 공변량 변화를 해결하기 위한 것이 배치 정규화이다. 배치 정규화는 **계층을 지날 때마다 매번 정규화**를 하여 내부 공변량 변화를 없앤다.(주로 뉴런의 가중 합산과 활성화 함수 사이에서 진행)
- 기존의 정규화와 다른 점은 **모델의 계층 형태로 데이터 정규화를 한다**는 점과 **전체 데이터가 아닌 미니 배치에 대해 정규화**한다는 점이다.
- 배치 정규화의 과정은 우선 **표준 가우시안 분포로 정규화**를 한 뒤, 학습 파라미터인 **$\gamma$와 $\beta$를 통해 원래 분포로 복원**하면 된다. 이 $\gamma$는 분포의 스케일링을 다루고, $\beta$는 분포를 translation을 해주는 역할이다.
- 이 $\gamma$와 $\beta$는 정해주는 것이 아니라, 학습을 통해 구하는 값이다. 초기 값은 각각 1과 0에 근사한 값으로 설정하여 시작한다. 즉, 단순, 표준화한 값에서 시작한다 볼 수 있다.
- 표준 가우시안 분포로 정규화를 하여 데이터가 계층을 지날 때마다 표준 가우시안 분포로 바뀌어 내부 공변량 변화를 최소화 하게 된다.
- 하지만, **표준 가우시안 분포는 모델이 비선형성을 제대로 표현할 수 없고**, 우리가 원하는 것은 표준화가 아닌 **입출력의 분포를 맞추는 것**이기에 원래 분포로 복구해주는 작업을 거친다.
- **표준 가우시안이 비선형성이 감소하는 이유**
    - 우선 정규화 이후에 활성화 함수를 거치는데 활성화 함수가 Sigmoid인 경우부터 보자
    - 정규화를 통해 데이터의 범위는 [-1, 1]이 될 것이다. 하지만 Sigmoid에서 해당 범위는 선형이기에 비선형성이 사라진다.
    - ReLU에 적용해보면 [-1, 1] 범위에서 절반은 0이 되어 학습이 어렵고, 나머지 절반도 선형적이므로 비선형성이 줄어든다.
    
    ![스크린샷 2025-04-15 175908](https://github.com/user-attachments/assets/cd74c792-33fc-4c66-9297-804d555be13e)

- 배치 정규화를 통해 **내부 공변량 변화가 억제되면**, 각 계층의 입력 분포가 안정되기 때문에 **그레디언트 흐름이 안정화되고 학습 속도와 성능이 향상**된다.
- 또한 데이터의 분포를 유지하려 하므로, **가중치 초기화의 의존도가 낮아지고 높은 학습률을 사용**할 수 있다.
- 추가적으로 미니 배치 별로 정규화하여 미니 배치의 조합에 따라 데이터가 조금씩 변형되어 **확률적 성질까지 부여**하여 성능이 올라간다.

- 위와 같은 내부 공변량 변화 외에도, 활성화 함수의 동작 관점에서도 배치 정규화가 필요한 이유가 있다. 활성화 함수의 입력이 극단적인 값으로 치우치면, 해당 층의 비선형성이 제대로 작동하지 않아 학습이 원활히 이루어지지 않는다.
- 활성화 함수를 거치기 전에 우리는 $\sum{wx}$를 계산하는데, 이 값은 우리가 가중치 초기화를 하더라도 어떻게 변화할 지 알 수 없다. 이 값이 극단적인 분포를 가질 경우, 문제가 발생할 수 있다.

<img width="600" height="800" alt="image" src="https://github.com/user-attachments/assets/28cb52b4-cb10-4d6e-851c-726e622ec2a8" />
  
- 예를 들어 Sigmoid함수를 사용할 때, $\sum{wx}$가 극단적인 음수 분포나 극단적인 양수 분포를 가지게 되었다고 생각해보면, 이 값들의 Sigmoid 값들은 0 혹은 1에 근사하고, 이에 따라 역전파 시에 **Vanishing Gradient문제가 발생**한다.
- ReLU의 경우에서는 극단적인 음수 분포에서는 **Vanishing Gradient문제**가 발생하고, 극단적인 양수 분포에서는 모든 기울기가 1이 되어 모두 동등하게 학습되기에 **특징 추출이 어려워 Overfitting**이 발생할 수 있다.
- 배치 정규화는 이러한 문제를 해결하기 위해, $\sum wx$ 값을 미니 배치 단위로 정규화하여 **활성화 함수가 적절한 비선형성을 발휘할 수 있는 범위로 입력을 재배치**해 준다. 이를 통해 각 층의 학습이 더 안정적으로 이루어질 수 있다.
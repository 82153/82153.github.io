---
layout: article
title: "[DL Basic] 딥러닝의 문제점 & 경사 하강법"
date: 2025-04-10 12:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---
## 딥러닝의 문제

- 딥러닝의 핵심은 **활성화 함수가 적용된 은닉층이 쌓이면서 비선형 영역을 표현**하는 것이다. 은닉층이 쌓이면서 데이터를 더 잘 분류할 수 있지만 다음의 문제점이 존재한다.

### 과적합 문제 발생

- 우리가 모델 학습 시에 사용하는 훈련 데이터는 현실 세계의 데이터 중 극히 일부이다. 이 훈련 데이터만을 과하게 학습하면 훈련 데이터에 대해서는 좋은 성능을 보이겠지만, 훈련 데이터에 포함되지 않은 데이터에 대해서는 오차가 증가할 수 있다.
    
    ![image.png](attachment:a663ec1f-13ee-4b26-b658-b2b9342a3bbe:image.png)
    
- 위와 같이 **과적합은 훈련 데이터를 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상**을 말한다.
- 이런 과적합 문제를 막기 위해, **DropOut, 모델 복잡도 줄이기, 데이터 셋 다양상 높이기, 규제, 조기 종료, 앙상블, 배치 정규화** 등의 방법을 사용한다.
- [https://82153.github.io/dl basic/2025/04/14/정규화.html](https://82153.github.io/dl%20basic/2025/04/14/%EC%A0%95%EA%B7%9C%ED%99%94.html) ← 위의 내용들을 정리한 것

### 기울기 소멸 문제

- 기울기 소멸 문제란 은닉층이 많아지면서 발생하는 문제로 **출력층에서 은닉층으로 전달되는 오차가 줄어들어 학습이 안되는 문제**이다.
    
    ![image.png](attachment:815aea71-2ecd-4069-82a7-a93eecbaa340:image.png)
    
- 역전파가 진행될 때, 그레디언트를 곱 연산을 통해 구한다. 이 때, 0에 가까운 수가 곱해져 **그레디언트가 0에 수렴하여 학습이 느려지거나 진행되지 않는 것**이다.
- 이 문제를 해결하기 위해, Sigmoid나 Tanh 대신 ReLU 함수를 사용하여 해결 할 수 있다.

### 성능이 나빠지는 문제 발생

- 은닉층이 많을수록 손실 함수의 형태가 **매우 고차원적이고 비선형적인 곡면**이 된다.
- 하지만 경사 하강법은 단순히 기울기가 0인 지점을 찾아가는데, 그곳이 기울기가 0일지라도 지역 최적해거나 안장점일 수도 있다.
- 또한 배치 경사 하강법은 전체 데이터를 한 번에 계산하므로 효율이 떨어지고, 업데이트가 느려진다.
- 이러한 문제를 해결하기 위해 **확률적 경사 하강법**과 **미니 배치 경사 하강법**을 사용한다.
    
    ![image.png](attachment:5481811e-a92a-4c7a-a2f1-002857860967:image.png)
    

### 배치 경사 하강법

- 배치 경사하강법은 **전체 데이터 셋에 대해 오류를 구한 뒤, 딱 한 번만 계산하여 파라미터를 업데이트**한다. 모든 데이터 셋에 대하여 계산하므로 **학습이 오래 걸린다.**(업데이트가 오래 걸린다는 것)

### 확률적 경사 하강법(SGD)

- SGD는 임의로 선택한 데이터 하나에 대해 기울기를 계산하는 방법으로 적은 데이터를 사용하기에 빠른 계산이 가능하다.
    
    ![image.png](attachment:e6a5159c-ee53-4bb6-813d-94343d39ffaf:image.png)
    
- 하지만 하나의 데이터만 보기에 배치 경사 하강법보다 파라미터의 **변경 폭이 불안정하고, 배치 경사 하강법보다 정확도가 낮을 수 있지만, 속도가 빠르다**는 장점이 있다.(업데이트가 빠르다는 것으로 전체적인 수렴이 빠르다는 뜻이 아니다.)

### 미니 배치 경사 하강법

- 미니 배치 경사 하강법은 **전체 데이터 셋을 여러 개의 미니 배치로 나눠, 미니 배치 1개마다 기울기를 구한 뒤 평균 기울기로 파라미터를 업데이트**를 한다.
    
    ![image.png](attachment:03783c26-a478-46d7-bc8d-35b7561b3eac:image.png)
    
- 미니 배치 경사 하강법은 **전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적**이다.
- **속도: 배치 경사 하강법 < 미니 배치 경사 하강법 < SGD**
- **안정성: SGD < 미니 배치 경사 하강법 < 배치 경사 하강법**
- 따라서 실제 딥러닝에서는 **SGD의 빠른 수렴 특성과 BGD의 안정성을 모두 고려한 미니 배치 방식**이 가장 많이 사용된다.
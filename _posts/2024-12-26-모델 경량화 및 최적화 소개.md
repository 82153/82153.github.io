---
layout: article
title: "모델 경량화 및 최적화 소개"
date: 2024-12-26 11:21 +0900
category: Lightweighting_Optimization
tags: [Lightweighting & Optimization]
aside:
  toc: true
sidebar:
  nav: layouts
---
## 경량화

- 최근 AI 모델은 점점 거대해지므로 이로 인해 계산 자원량이 기하 급수적으로 증가했다.
- 이런 거대한 AI 모델들을 다양한 환경에서 사용하기에는 다양한 문제들이 발생하기에 경량화가 필요하다.
- 모델 경량화란 **AI 모델의 크기를 줄이고 계산 비용을 감소시키면서도, 필요한 모델의 성능을 최대한 유지하는 기술**을 말한다.
- 대표적으로 **Pruning(가지치기), Knowledge Distillation(지식 증류), Quantization(양자화)** 등이 있다.
- 경량화가 필요한 자세한 이유
    1. **실시간 처리**: 데이터에 반응하여 즉각적인 결정이나 예측이 중요한 분야(자율 주행, 헬스 케어 등)에서 필요하다.
    2. **저자원 효율**: 온디바이스 모델에 탑재하기 위해서는 모델이 가볍고 추론 가속화가 필요하다.
    3. **에너지 효율성**: 모델 경량화를 통해 에너지 소비를 줄이기 위해 필요하다.

## 모델 경량화 기법

- **Pruning(가지치기)**
    - 학습된 모델에서 중요도가 낮은 뉴런이나 연결을 **제거하여** 모델의 크기와 계산 비용을 줄이는 기법이다.(→중요도 판단은 어떻게 하는지.. -> 내 생각으로는 weight나 node의 절대값이 0에 가까운 걸 하는게 좋을듯 싶다. 어차피 0 곱하면 없어지니까 그럴듯 싶다.)
    - Pruning에서도 뉴런, 채널 혹은 레이어 전체를 제거하는 **Structured Pruning**과 연결 만을 제거하는 **Unstructured Pruning**이 있다.
- **Knowledge Distillation(지식 증류)**
    - 고성능의 Teacher 모델로부터 **지식을 전달** 받아서 Student 모델을 학습 시키는 기법이다.
    - 공개된 모델을 Teacher로 사용하는 **White-box KD**와 비공개 모델을 Teacher로 사용하는 **Black-box KD**가 있다.
- **Quantization(양자화)**
    - 모델의 가중치와 활성화를 낮은 비트 정밀도로 변환하여 저장 및 계산 효율성을 높이는 기법이다.

## 모델 경량 재학습

- AI 모델이 커짐에 따라, Fine-Tunning 같이 모든 파라미터를 업데이트하는 방식은 비용 문제가 커지고 있다.
- **PEFT(Parameter-Efficient Fine-Tuning)**
    - 훈련된 모델을 **자원 효율적인 방식으로 재학습**하는 방식을 말한다.
    - 최대한 적은 파라미터를 학습하며 Fine-Tunning과 비슷한 성능을 내는 것이 목표이다.
    - 대표적으로 기존 가중치는 고정하고 layer 사이에 새로운 layer를 추가하여 해당 부분만 학습하는 **Adapter**, 저차원 행렬을 사용하여 사전 학습된 모델의 일부만 효율적으로 학습하는 **LoRA** 등의 기법이 있다.

## 모델 메모리 최적화

- 최근 AI모델은 굉장히 크기에 하나의 GPU로는 학습 및 추론이 불가능하다. 그렇기에 메모리 측면에서도 최적화가 필요하다.
- **Distributed Training**
    - 모델이나 데이터를 여러 개의 GPU로 분산시켜 병렬화 하는 기법이다.
    - **Data Parallelism**
        - 큰 데이터를 여러  GPU에 분할하여 동시에 처리함으로써 학습 속도를 높이는 기법이다.(근데 결국 모델이 커서 GPU에 안들어가면 의미 없는거 아닌가…)
    - **Model Parallelism**
        - 큰 모델을 여러 GPU에 분할함으로써 하나의 GPU로는 처리할 수 없는 대형 모델도 처리 가능하게 하여 메모리 효율성을 증가시키는 기법이다.

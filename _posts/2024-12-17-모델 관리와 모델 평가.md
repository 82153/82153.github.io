---
layout: post
title: "[MLflow]MLflow를 이용한 모델 관리 및 모델 평가"
date: 2024-12-17 21:19 +0900
category: Product_Serving
tags: [MLflow]
---
## 모델 관리

- 우리는 모델링을 할 때, 많은 시행착오를 거친다. 이때, 가장 좋은 성능을 가진 모델은 어떤 것이고, 모델 성능, 모델 생성일, 모델 메타 데이터 등을 관리할 필요가 있다.
- 또한 여러 모델을 운영할 때에도 관리가 필요하다. 이런 모델 관리를 통해 다른 사람들도 모델을 사용할 때, 이러한 것들을 알면 더 잘 사용할 수 있을 것이다.
- 아래의 표는 모델 관리 시에 기본적으로 신경 쓰는 것이 있다.

| **모델 메타 데이터** | 언제 만들어졌고, 어떤 데이터를 사용했는지와 성능 등 |
| --- | --- |
| **모델 아티팩트** | 모델의 학습된 결과물(pickle, joblib) |
| Feature / Data | 모델을 위한 Feature, Data |

## MLflow

- 모델 관리를 위한 대표적인 오픈 소스이다.
- **핵심 기능**
    1. **Experiment Management & Tracking**
        - 실험들을 관리 및 실험 내용을 기록할 수 있다.
        - 또한 실험 정의, 실행도 가능하고 소스 코드나 하이퍼 파라미터 등도  저장 가능하다.
    2. **Model Registry**
        - MLflow로 실행한 모델을 Model Registry에 등록할 수 있다.(저장될 때마다 버전이 자동으로 업데이트됨)
        - 이를 통해 다른 사람들과 쉽게 공유 및 활용할 수 있다.
    3. **Model Serving**
        - Model Registry에 등록된 모델을 REST API 형태로 Serving할 수 있다.
- **핵심 요소**
    1. **Tracking**
        - 머신 러닝 코드 실행과 파라미터, 코드 버전, Metric등의 로깅을 위한 API이다.(즉, 기록을 해준다는 것)
        - 웹 UI 또한 제공해준다.
    2. **Model Registry**
        - 모델 관리를 위한 체계적인 접근 방식을 제공한다.
        - 태그, 별칭 지정, 버저닝, 계보를 포함한 모델의 전체 수명 주기를 관리한다.
    3. **Project**
        - 코드, Workflow, Artifact의 패키징을 표준화하는 것으로 재현이 가능하도록 관련 내용을 모두 포함하는 개념이다.

## 모델 평가

- **OFFLINE**
    - 단순하게 과거 데이터를 기반
    - 주기성 혹은 배치 / 훈련 후 정적인 모델 평가
    - 우리가 local에서 알고리즘 개발할 때 대부분이 Offline이다.
- **ONLINE**
    - 복잡하고 실시간 데이터에 기반
    - 즉각적인 성능 평가 / 모델의 동적인 변화에 빠르게 대응
    - 배포 후에 실제 데이터가 들어올 때 평가하는 것이다.
- Offline 모델 평가에는 **Hold-out Validation, K-fold cross Validation, Boostrap resampling** 등이 있다.
- **ONLINE 모델 평가**
    - **AB-Test**
        
        ![스크린샷 2024-12-17 210606](https://github.com/user-attachments/assets/25bd6c8b-6c1a-4b8a-9b5b-7b6f156dd37a)
        
        - 같은 양의 트래픽을 2개 이상의 버전에 전송하고 예측을 얻어 비교 및 분석을 한다.
        - 통계적 유의미성을 얻기까지 매우 오래 걸려서 Multi-Armed Bandit과 같은 최적화 기법을 같이 사용하기도 한다.
    - **Canary Test**
        
        ![스크린샷 2024-12-17 210923](https://github.com/user-attachments/assets/7f131e2f-593a-46e3-9ec9-ab99da9d5351)
        
        - 새로운 모델로 트래픽이 들어가도 문제가 없는지 체크하는 것이다.
        - AB-Test와 달리 50:50으로 나눠서 확인하는 것이 아닌 최신 버전에 더 낮은 비율(ex. 90:10)으로 설정하고, 괜찮으면 점진적으로 비율을 늘려간다.
    - **Shadow Test**
        
        ![스크린샷 2024-12-17 211303](https://github.com/user-attachments/assets/6cc535cd-a883-4087-905f-a31c945f375a)
        
        - Production과 같은 트래픽을 새로운 버전의 모델에 적용
        - 모든 트래픽을 기존의 시스템에 주어 서빙된 모델의 영향을 최소화한다.
        - 같은 트래픽을 새로운 버전에도 전송하여 결과물을 비교한다.
        - 이때 client에게 주어지는 것은 기존의 버전의 결과만이 주어진다.

---
layout: article
title: "[DL Basic] 인공지능, 머신러닝, 딥러닝"
date: 2025-04-08 12:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---
## 인공 지능, 머신 러닝, 딥러닝

### 인공지능이란

- 간단하게 인공지능과 머신 러닝, 딥러닝을 각각 정리해보면, **인공지능은 인간의 지능을 모방하여 사람이 하는 일을 컴퓨터가 할 수 있게 하는 기술**이다.
- 이 인공 지능의 방법으로 머신 러닝과 딥러닝이 있는 것이다. 관계를 정리해보면 인공 지능 안에 머신 러닝이 있고, 머신 러닝 안에 딥러닝이 포함되는 관계이다.→ **인공지능 > 머신러닝 > 딥러닝**
    
    ![스크린샷 2025-04-18 174208](https://github.com/user-attachments/assets/31488ac1-d7bd-46b8-a0cb-807f2e2f7a72)
  
- 여기서 머신러닝과 딥러닝 모두 학습 모델을 제공하여 데이터를 분류하는 기술이다. 이 둘은 **특징 추출을 사람이 하느냐 모델이 스스로 하느냐의 차이**가 있다.
- **특징 추출이란 데이터를 분석하여 패턴이나 규칙을 찾아, 컴퓨터가 인지할 수 있는 데이터로 바꿔주는 작업**이다.
    
    ![스크린샷 2025-04-18 174854](https://github.com/user-attachments/assets/4f422cb9-50cc-41b6-aabf-e7763d7b3b3f)

- 머신러닝의 경우, 데이터의 대해 전처리를 하여, 컴퓨터가 데이터를 이해할 수 있게 해줘야 한다. 머신러닝은 데이터의 특징을 스스로 추출할 수 없기에 인간이 전처리를 통해 처리해줘야 한다.
- 반면에 딥러닝은 인간이 전처리 해주는 작업이 필요없다. 딥러닝은 대량의 데이터를 통해 스스로 분석하여 특징을 스스로 추출한다.
    
    ![스크린샷 2025-04-18 174924](https://github.com/user-attachments/assets/4720a533-37bf-4459-86a6-c0d087197bf0)


---

## 머신러닝

- 머신러닝은 컴퓨터가 스스로 대용량 데이터에서 지식이나 패턴을 찾아 학습하고 예측을 수행하는 것이다.

### 머신러닝의 학습 과정

- 머신 러닝의 학습 과정은 크게 **학습 단계와 예측 단계**로 나뉜다.
    1. **학습 단계**: 훈련 데이터를 머신 러닝 알고리즘에 적용하여 학습시키고, 이 학습 결과로 모형이 생성된다.
    2. **예측 단계**: 학습 단계에서 생성된 모형에 새로운 데이터를 적용하여 결과 예측
    
    ![스크린샷 2025-04-18 184355](https://github.com/user-attachments/assets/522ec427-dd91-4a2a-b836-0c00fce5e3d0)


### 머신 러닝의 주요 구성 요소

- **데이터**
    - 실제 현상의 특성을 잘 파악하기 위해 좋은 훈련 데이터가 필요하다. 또한 편향되지 않는 데이터를 확보해야한다.
    - 학습 시에는 전체 데이터 셋을 학습 데이터셋(80%), 테스트 데이터셋(20%)으로 나눠서 사용한다.  여기서 학습 데이터 셋을 또 나눠 학습 데이터셋과 검증 데이터셋으로 사용하기도 한다.
- **모델**
    - 학습 단계에서 얻은 최종 결과물로 가설이라고도 한다. 모델이 학습하는 과정은 **모델 선택 → 모델 학습 및 평가 → 평가를 바탕으로 모델 업데이트** 순으로 진행 된다. 이 과정을 최종적으로 거친 후에 모델을 해결하고자 하는 문제에 적용하여 결과를 낸다.

### 머신러닝 학습 알고리즘

- **지도 학습**
    - **정답(레이블)을 주고 학습**시키는 방법이다. 지도 학습은 **분류, 회귀 유형의 작업**에서 자주 사용된다.
    - 분류에는 **KNN, SVM, 결정 트리, 로지스틱 회**귀 등의 알고리즘을 사용한다.
    - 회귀에는 **선형 회귀** 등의 알고리즘을 사용한다.
- **비지도 학습**
    - **정답(레이블)을 주지않고** 특징이 비슷한 데이터를 클러스터링하여 예측하는 학습 방법이다. 지도 학습과 달리 명확한 분류가 아닌 유사도 기반으로 특징이 유사한 것끼리 묶어서 분류한다.
    - 비지도 학습에서의 작업은 **클러스터링과 차원 축소**가 있다. 클러스터링에는 **K-means 클러스터링, DBSCAN**이 있고, 차원 축소에는 **PCA**가 있다.
- **강화 학습**
    - 강화 학습은 자신의 행동에 대해 보상을 받으며, 이 보상을 최대화 시키는 식으로 학습한다. 여기서, 행동을 취하는 객체를 Agent, 이 Agent가 속한 세계를 환경이라 한다.
    - 이 Agent가 특정 환경에 따라 다른 행동을 취하여 보상을 받는다.

---

## 딥러닝

- 딥러닝은 **인간의 신경망을 모방한 것**으로 머신 러닝의 일종이다. 또한 학습 과정에서 **신경망과 역전파**가 핵심이다.  신경망은 퍼셉트론으로부터 나온 것으로 **입력층, 은닉층, 출력층**으로 구성되어 있다. 여기서 **은닉층이 여러 개 쌓인 것을 심층 신경망 혹은 딥러닝**이라고 한다.
- **딥러닝 학습 과정**
    1. **데이터 준비**
        - 파이토치, 케라스에서 제공하는 데이터셋(전처리됨)이나 캐들 같은 공개 데이터(전처리X)를 사용
    2. **모델 정의**
        - 신경망을 생성하는 단계이다.
        - 주로 은닉층의 개수와 성능은 비례하지만, 과적합 확률 또한 비례하기에 신경망을 잘 설계하는 것이 중요하다.
    3. **모델 컴파일**
        - **활성화 함수, 손실 함수, 옵티마이저**를 선택하는 단계이다.
        - 내가 해결하고자 하는 문제(분류 or 회귀)에 따라 손실 함수가 달라지고, 활성화 함수와 옵티마이저를 잘 설정하면 과적합을 피할 수 있다.
    4. **모델 훈련**
        - 모델을 학습시키는 단계로 **에포크, 배치 사이즈, 학습률** 등을 정하는 단계이다.
        - 여기에서 **에포크는 전체 dataset을 한번 순회하는 단위**이고, **배치는 훈련 데이터셋을 나눈 묶음**이다.
        - 이 **배치가 모델이 한 번에 처리할 데이터 양을 나타내고, 업데이트의 단위**이다. 이 배치 사이즈가 크면, 학습 속도(정확히는 업데이트 속도)가 느려지고, 메모리가 부족할 수 있다.
    5. **모델 예측**
        - 검증 데이터셋을 모델에 적용하여 **실제로 예측을 진행해보는 단계**이다.
        - 이때 성능이 낮다면 파라미터를 튜닝하거나 모델을 재설계한다.
        - 여기서 성능은 정확도, 훈련 속도, 연산량 등을 지표로 볼 수 있고, 정해진 것이 아니라 다양한 의미로 사용된다.
- **딥러닝학습 알고리즘**
    - **지도 학습**
        - 머신러닝의 지도학습과 동일하게 데이터와 같이 레이블이 주어진 상태로 학습하는 것이다.
        - 컴퓨터 비전 쪽에서는 **CNN**을 주로 사용한다. 이 CNN을 통해 이미지 분류, 이미지 인식(Object Detection), 이미지 분할(Segmentation) 등의 작업을 수행한다.
        - 시계열 데이터(언어 등)를 활용할 때에는 **순환 신경망(RNN)**을 사용한다. 이후에는 LSTM등의 모델도 사용한다.
    - **비지도 학습**
        - 비지도 학습은 **레이블이 주어지지 않은 상태**로 학습하는 것이다. 비지도 학습에는 text를 딥러닝 모델이 다룰 수 있게 벡터화하는 **워드 임베딩**과 **클러스터링**이 있다.
    - **전이 학습**
        - **사전에 학습이 완료된 모델을 가지고 우리가 원하는 학습에 미세 조정 기법을 사용하여 학습**하는 방법이다. → 당연히 사전 학습된 모델이 필요
    - **강화 학습**
        - 위에서 설명한 머신러닝에서의 강화학습과 동일하다.
- **순전파(Forward)와 역전파(Backward)**
    - 딥러닝은 **순전파(Forward Propagation)**를 통해 예측값을 도출하고, 이 예측값과 실제값의 차이인 **손실(Loss)**을 계산한 후, 손실을 줄이기 위해 **역전파(Backpropagation)**를 통해 가중치를 업데이트한다. -> 이 과정을 **경사 하강법(Gradient Descent)라고 한다.**

    - 순전파는 학습 데이터가 신경망에 입력될 때 발생하며, 전체 네트워크를 거쳐 예측 결과를 계산하는 과정이다.

    - 간단히 말하면, 입력값(input)과 가중치(weight)의 가중합을 계산하고, 여기에 편향(bias)을 더한 뒤 활성화 함수를 적용해 출력값을 얻는다. → 입력층 → 은닉층 → 출력층
        
        ![스크린샷 2025-04-18 205205](https://github.com/user-attachments/assets/c09449d2-39a2-4412-8499-7363a080e162)

    - 여기서 weight는 입력 값이 결과에 얼마나 영향을 미치는지를 조절하는 파라미터이고, bias는 이 가중합 결과에 일정한 값을 더해주는 상수 항으로서 출력값을 조정하는 역할을 한다.

    - 위 그림처럼 $x_0$을 1로 고정하고, $w_0$를 bias로 함께 계산하는 방식도 자주 사용된다.
        
        ![스크린샷 2025-04-18 210120](https://github.com/user-attachments/assets/5434498a-f63c-4cb9-b1b9-7bce4cbff578)

    - 역전파는 실제로 모델을 학습시키는 핵심 과정이다. 출력값과 실제값 사이의 손실을 계산하고, 이 손실을 각 가중치에 대해 편미분한 값을 기반으로 학습률을 곱해 가중치를 손실이 줄어드는 방향으로 조정한다. → 출력층 → 은닉층 → 입력층

    - 이 과정은 은닉층의 모든 뉴런에 전파되며, 각 뉴런의 가중치는 출력값에 기여한 정도에 따라 조정된다.
    - $w_{i}^{new} = w_{i} - \rho \nabla \frac {Loss}{w_{i}}$를 통해 가중치를 업데이트해가면서, Loss를 줄여간다.
        
        ![스크린샷 2025-04-18 211404](https://github.com/user-attachments/assets/3eb62028-f5b0-45d8-b702-49a3b417856d)

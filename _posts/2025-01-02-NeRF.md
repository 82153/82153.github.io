---
layout: article
title: "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"
date: 2025-01-02 16:21 +0900
category: Paper_Review
tags: [Paper_Review]
aside:
  toc: true
sidebar:
  nav: paper_review
---
### Abstract

![스크린샷 2025-01-02 010747](https://github.com/user-attachments/assets/b3eeac67-bdc8-4ada-9a51-99d09a244893)
- 몇개의 input view를 사용하여 Underlying continuous volumetric scene function을 최적화하여 **복잡한 장면의 새로운 뷰를 합성**하는 SOTA 기술을 제시
- Convolution없이 **Fully-Connected**만 사용하고, Input으로는 **5D($x, y, z, \theta, \phi$)**를 사용하고, output으로는 해당 위치의 **Volume Density**와 **view-dependent emitted radiance**가 나온다.
- $x, y, z, \theta, \phi$들이 input으로 들어오고, $x, y, z$는 공간 위치를 나타내고, $\theta, \phi$는 관측 방향을 나타낸다.($\theta$는 수평 방향, $\phi$는 수직 방향)
- Camera Ray에 따라 5D를 쿼리하고, Classic한 Volume Rendering을 통해 output 색과 밀도를 이미지로 투영한다.
- Volume Rendering을 쓰는 이유?
    - Volume Rendering은 미분이 가능하기에 전체적인 Process에 대해서 한번에 역전파가 가능해진다.
    - 우리가 Representation을 최적화하기 위해 우리가 알고 있는 카메라 위치(포즈)의 이미지만 알고 있으면 된다.

### Introduction

- NeRF 연구에서는 **연속 5D 장면 표현의 매개변수를 직접 최적화하여 캡처된 이미지 집합의 렌더링 오류를 최소화**하는 새로운 방식을 제시
- 즉, 5D표현된 것을 input으로 MLP에 통과시켜 각 지점에서 어떤 빛이 방출되고, 해당 지점이 얼마나 불투명한지(밀도) 를 구하는 것이다.
![스크린샷 2025-01-01 202753](https://github.com/user-attachments/assets/6dc6daca-69ff-471b-83c5-f9b0b0bd76fe)
- 특정 시점의 neural radiance field를 rendering하기 위한 3단계
    1. 카메라 광선을 장면을 통과시키며 샘플링된 3D 점들을 생성
    2. 이 점들과 해당하는 2D 관찰 방향을 신경망에 입력하여 색상과 밀도의 출력 세트를 생성
    3. 고전적인 볼륨 렌더링 기법을 사용하여 이 색상과 밀도를 2D 이미지로 축적
- **정리하면 Camera Ray를 물체에 통과시켜 $x, y, z, \theta, \phi$를 얻고 이 값을 MLP에 입력하여 R, G, B와 밀도를 출력한 뒤, 이 색상과 밀도를 Volume Rendering을 누적하여 2D 이미지를 생성한다.**
- 기존의 neural radiance field representation은 **고해상도에 대해서 충분히 수렴하지 못하고, 요구되는 Camera Ray 별 샘플의 수가 비효율적**이라는 문제가 있음 **→ 기존 방법의 문제점**
- **MLP**가 **고주파 함수**들을 표현할 수 있도록 **Positional Encoding**을 사용하여 입력 5D 좌표를 변환. → **고해상도의 수렴 문제 해결방안**
- 이 **고주파 장면 표현**을 적절히 샘플링하기 위해 필요한 쿼리 수를 줄이는 **계층적 샘플링** 절차를 제안. → **샘플 수 관련 해결 방안**
- 위와 같은 방법은 Volumetric Representation에서도 장점이 있다
    - 복잡한 실제 세계의 기하학과 외관을 표현할 수 있으며, 투사된 이미지를 사용한 그래디언트 기반 최적화에 적합
    - 기존의 복셀 기반 방법보다 더 적은 저장 공간
- 요약하자면
    - **복잡한 기하학과 재료**를 가진 연속적인 장면을 **5D neural radiance fields**로 접근하여 , 기본적인 MLP 네트워크로 **파라미터화**된다.
    - **기존의 볼륨 렌더링 기법**을 바탕으로 한 **미분 가능한 렌더링 절차**. 이를 통해 우리는 표준 RGB 이미지에서 이 표현을 최적화합니다. 여기에는 **계층적 샘플링 전략**이 포함되어, MLP의 용량을 **장면 내용이 보이는 공간**에 할당할 수 있게 합니다.
    - **위치 인코딩**을 사용하여 각 입력 5D 좌표를 **더 높은 차원의 공간**으로 변환합니다. 이를 통해 우리는 **고주파 장면 콘텐츠**를 나타내기 위해 신경 방사 필드를 성공적으로 최적화할 수 있습니다.

### Neural Radiance Field Scene Representation

- Input으로 **Location (x)**와 **View Direction (d)**를 받아, Output으로 **Color (c)**와 **Volume Density ($\sigma$)**를 계산한다.
- $\sigma$는 위치(x)에 의존적이며, **x를 256 채널과 ReLU 활성화를 사용하는 8개의 Fully-Connected Layer(FC)를 거쳐 계산**한다.
- 이후 $\sigma$를 계산한 결과에서 **특징 벡터를 추출**하여, 관찰 방향(d)와 **Concatenation**한 후 **128 채널과 ReLU를 사용하는 추가적인 FC Layer**를 통해 시점 의존적인 색상(c)을 얻는다.
- **$\sigma$: 위치 x에만 의존**, **c: 위치 x와 시점 방향 d 모두에 의존**한다.
    - 밀도($\sigma$)는 바라보는 방향에는 관계가 없으니, 위치 정보만 존재하면 되고, 색상($c$)는 위치 정보도 중요하지만 바라보는 방향에 따라 달라질 수 있기 때문이다.
- 관찰 방향(d)는 고유의 128차원 임베딩 공간으로 매핑되며, 이 과정은 시점 의존적 색상(c)을 보다 효과적으로 표현하기 위한 것이다.

![스크린샷 2025-01-01 204932](https://github.com/user-attachments/assets/fd23a9f9-b332-4926-98bd-a4d7e1c81dbb)

- 처음 input으로는 3D 공간 위치 정보(x, y, z)이 입력으로 들어오고, 5번째 Layer에서 Skip Connection과 같이 (x, y, z)을 concat해준다.
- 이후 8번째 Layer에서 밀도 값을 추출하고, 여기에 방향 정보($\theta, \phi$)가 입력되어 마지막 Layer를 거치고 Sigmoid를 통해 RGB값이 나온다.
- 이 Representation은 최종적으로 **Volume Rendering 과정과 결합하여 2D 이미지를 생성**하는 데 사용된다.

### Volume Rendering with Radiance Fields

- 위의 MLP를 통해 얻은 결과값들을 결합하여 하나의 픽셀로 변환하는 과정을 거치는데 아래의 수식을 통해 Rendering된 RGB값을 구한다.

![스크린샷 2025-01-01 220937](https://github.com/user-attachments/assets/0ff5ab34-5eec-4339-af9a-f2934401b5c7)

- $r(t)$: Ray를 나타는 식으로 $o+td$로 구한다.(o는 카메라 초점 위치, t는 이동 거리, d는 방향)
- $\sigma(r(t))$: 특정 지점 t에서의 **밀도**
- $c(r(t), d)$: 특정 방향 d에서의 t 지점의 **색상**
- $T(t)$: $t_n$에서 $t$지점까지의 밀도를 모두 더한 것의 음수를 취한 것으로 **누적 투과도**를 나타낸다.
- 위 식을 좀 더 자세히 설명하면, Ray 상의 각 Point들의 색상의 가중합이라고 볼 수 있는데, $\sigma(r(t))$와 $T(t)$를 가중치로 보면, **밀도가 높고, 해당 Point 이전의 점들의 밀도가 낮은 부분에 대해 높은 가중치를 주는 것**이다.
- 여기에서 Ray들의 점들은 이론 상 무한 개이기에 적분이 불가능하다.  그렇기에 sampling을 진행하는데 무작위로 뽑으면 특정 구간에서만 많이 뽑히는 문제가 발생한다.

![스크린샷 2025-01-01 235119](https://github.com/user-attachments/assets/e6e98cbc-f723-4287-88f3-bc69ad525b7d)

- 이를 위해 **Stratified Sampling**을 통하여 Ray의 구간을 N등분하고, 각 구간 내에서 뽑힐 확률을 균등하게 하여 랜덤하게 추출한다.
- 이후 Optical Models for Direct Volume Rendering 논문의 quadrature rule을 통해 위와 같은 수식으로 변화시킨다.

![스크린샷 2025-01-02 000012](https://github.com/user-attachments/assets/e3caf768-4a3b-4a1d-a7a4-223c1b4a935a)

- $T_i$는 i번째 샘플에 대한 **누적 투과도**
- $\sigma_i$는 i-번째 샘플의 **밀도**
- $c_i$는 i-번째 샘플에서의 **색상**
- $\delta_i$는 i번째 샘플과 i+1번째 **샘플 간의 거리**.

### Optimizing a Neural Radiance Field

- 위와 같은 방법으로는 SOTA 성능을 달성하기에 충분하지 않았다.
- 고해상도 표현을 하기 위해 Positional Encoding과 Hierarchical Sampling을 하였다.
1. **Positional Encoding**
    - $xyz\theta\phi$만으로 MLP를 학습 시키는 것은 공간적, 색상 변화가 급격한 곳에서는 성능이 안 좋은 것을 확인했다.
    
    ![스크린샷 2025-01-02 015604](https://github.com/user-attachments/assets/c5e901b6-2621-4595-b9cf-1791d5bd1632)
    
    - On the spectral bias of neural network을 통해 Deep network가 low-frequency를 학습하는데 편향되어 있다는 것을 확인 했고, 이를 해결하지 위해서는 데이터를 신경망에 전달하기 전에 High-frequency 함수를 사용하여 **입력을 더 높은 차원의 공간으로 매핑**해야, 데이터의 High-frequency를 더 잘 Fitting 시킬 수 있다는 점을 발견했다.
    - 이를 바탕으로 $xyz\theta\phi$ 각각에 Positional Encoding을 추가하여 입력 차원을 증가시키고 -1과 1 사이로 정규화를 하게 된다. (논문 실험에서는 $x, y, z$에는 L을 10, $\theta, \phi$에는 L을 4로 사용)
        
        ![스크린샷 2025-01-02 013140](https://github.com/user-attachments/assets/72992465-85cd-4790-b605-d223ef9420e2)
        
    - 일반적으로 우리가 아는 Transformer에서의 Positional Encoding은 Token의 위치를 알기 위해 사용하지만, NeRF에서는 연속적인 **입력 좌표를 고차원 공간으로 변환하여 MLP의 성능을 개선하는 데 사용**한다.
2. **Hierarchical Sampling**
    - NeRF에서는 Sampling 시에 단일 네트워크를 사용하는 것이 아니라 **Coarse 네트워크**와 **fine 네트워크** 2개를 사용한다.
    - Coarse 네트워크는 우리가 위해서 **Stratified Sampling**을 통해 추출한 Sample들로 학습한 것이다. 이것을 바탕으로 한 번 학습을 진행하여, Ray 상의 각 Point(i)들의 $T(1-exp(-\sigma\delta))$(이하 $w$)를 얻을 수 있다.
    - 이 $w$를 바탕으로 $w/sum(w_i)$들을 구하여 이 값으로 PDF를 구하고, Inverse Transform Sampling을 통해 밀도가 높은 부분에 대하여 추가적인 샘플링을 진행한다.
    - 이렇게 샘플링 한 것과 **Stratified Sampling**를 통해 샘플링한 것을 합쳐, fine 네트워크를 학습한다. **결론적으로는 Fine 네트워크를 사용한다.**
    
    ![스크린샷 2025-01-02 015604](https://github.com/user-attachments/assets/a1972966-9222-44c4-9fc3-9fdc51f76240)
    
    - 이를 통해 새로운 시점의 이미지의 최종 Loss는 각 픽셀의 색상을 비교한 아래와 같다.
    
    ![스크린샷 2025-01-02 021114](https://github.com/user-attachments/assets/ae42414b-04d2-49c7-9faa-9c93b01d59ee)
    
    - 최종적으로 Fine 네트워크를 사용하는데 왜 Coarse 네트워크의 값이 추가 되어 있을까?
        - **coarse 네트워크**는 장면에 대해 대충 예측을 하면서 어떤 부분이 중요한지 알려주고, **fine 네트워크**는 그 정보를 바탕으로 더 세밀한 예측을 한다고 한다.
        - PDF가 부정확할 경우 중요하지 않은 부분을 샘플링할 수 있어, Coarse 네트워크 학습도 포함시킨 것으로 보인다

### Conclusion

![스크린샷 2025-01-02 112322](https://github.com/user-attachments/assets/9b27492d-e4a0-47d8-8f3c-06350eebf1e5)

- 이전 방법들과 달리 CNN이 아닌 MLP를 사용하고 기존보다 높은 성능을 보임
- NeRF의 전체적인 프로세스와 사용한 기법들의 효과는 다음과 같다.
    
    ![스크린샷 2025-01-02 113327](https://github.com/user-attachments/assets/1b9defca-822e-447a-8a98-e854ea7397fd)
    
    - **Volume Rendering** -> **전체 프로세스가 미분 가능**하도록 설계되어 효율적인 학습 가능
    - **Stratified Sampling** -> **장면 연속성을 자연스럽게 유지**하며 고품질 이미지 렌더링
    - **Positional Encoding** -> **High Frequency한 부분**에 대해서도 렌더링 품질 향상
    - **Hierarchical Sampling** -> **샘플링 효율성을 극대화**하며 **중요 영역에서의 렌더링 품질 향상**
- NeRF에서도 한계점이 있는데 아래와 같다.
    1. 훈련 및 렌더링 속도가 느리다.
    2. 정적인 장면에 대해서만 성능이 좋다.
    3. 같은 환경에서 촬영한 이미지에 대해서만 성능이 좋다.
    4. 한 객체에 대해 학습된 모델은 한 객체만 렌더링 할 수 있다.
    5. Hierarchical Sampling을 사용했지만, 최적화 및 렌더링 효율을 개선할 필요가 있다.
    6. 기존 Voxel 및 Mesh 방식 대비 렌더링 품질 분석 및 실패 사례 해석이 어려움이 있다.

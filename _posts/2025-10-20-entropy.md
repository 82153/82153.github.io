---
layout: article
title: "[Mathematics] 정보 엔트로피"
date: 2025-10-20 12:54 +0900
category: Mathematics
tags: [Mathematics]
sidebar:
  nav: mathematics
---
### 정보 엔트로피

- 정보 엔트로피는 어떤 확률 분포를 가진 사건의 **불확실성을 측정하는 값**이며, 그 사건을 표현하기 위해 **필요한 최소 평균 정보량**이다.
- 확률이 높은 사건은 자주 나타나므로, 이를 구분하거나 전달하기 위해 필요한 정보량(비트 수)은 작아도 충분하다. 반대로, 확률이 낮은 사건은 드물게 나타나므로, 이를 명확하게 표현하기 위해서는 더 많은 정보량이 필요하게 된다.
- 따라서 확률이 낮을수록 정보량은 크고, 불확실성(엔트로피)도 높아지며, 확률이 높을수록 정보량과 불확실성은 낮아집니다.
- 이제 정보 엔트로피의 수식을 보면, 아래 수식으로 계산된다.

$$
H(x) = E[I(x)] = -\sum{P(x)log_{2}{}P(x)}
$$

- 즉 위의 정의대로 $E[]$는 기댓값을 나타내고, $I(x)$는 x에 대한 bit단위의 정보량을 나타낸다. 이 2개를 합쳐 bit 단위의 정보량의 기대값으로 볼 수 있다.

---

### Binary Entropy

- 이진 엔트로피는 **하나의 확률변수가 두 가지 값 중 하나**를 가질 때, 해당 변수의 불확실성 또는 정보량을 나타낸다.
- 여기에서 어떤 사건이 발생할 확률을 $p$라 할 때, 사건이 발생하지 않을 확률은 $(1 - p)$가 된다. 이 때, 이진 엔트로피는 아래와 같이 계산된다. 이는 단순히 위의 엔트로피를 계산했을 때, 이진인 경우로 계산한 것과 동일하다.

$$
H(x)=-plog_{2}p-(1-p)log_{2}(1-p)
$$

- 이진 엔트로피에서는 $p$가 0인 경우에는 $-(1-p)log_{2}(1-p)$부분만 살아남아 0이 된다. 이와 마찬가지로 $p$가 1인 경우에는 $-plog_{2}p$부분만 살아남아 마찬가지로 0이 된다.
- $p$가 0 또는 1이라는 것은 해당 사건이 무조건 발생하거나, 무조건 발생하지 않는다는 것이다. 이는 해당 사건의 불확실성이 없고, 필요한 정보량이 없다는 것이다.
- $p$가 0.5인 경우 실제로 계산하면 $H(0.5) = -0.5\log_{2}(0.5)-0.5\log_{2}(0.5)=1$이다. $p$가 0.5라는 것은 각 사건이 발생할 확률이 동등하다는 것이고, 이는 가장 불확실한 상황이고, 정보량이 가장 많이 필요하다는 것이다.
- 다만 이진 엔트로피는 단일 확률 분포의 불확실성을 측정하는 값이므로, 그대로는 **딥러닝에서 Loss로 사용되지는 않는다.** 딥러닝에서는 정답 분포와 모델의 예측 분포를 비교해야 하므로, Binary Cross Entropy나 Cross Entropy가 Loss로서 사용된다.

---

### Cross Entropy

- 위에서 본 엔트로피는 하나의 확률 분포에 대해서만 다뤘지만, Cross Entropy는 **서로 다른 확률 분포를 다룬 엔트로피**이다. 그렇기에 머신러닝 분류에서 정답 분포와 예측 분포를 다뤄, Loss로서 주로 사용된다.
- $P(x)$를 정답 분포, $Q(x)$를 예측 분포라 했을 때, 교차 엔트로피는 아래와 같다.

$$
H(P,Q)=-\sum{P(x)logQ(x)}
$$

- $P(x)$를 기준으로 $Q(x)$가  얼마나 유사한가를 나타낸다. 따라서 $H(P,Q)$와 $H(Q,P)$의 값은 다르다.
- 교차 엔트로피의 범위는 $[0,\infty]$를 가진다. 0일때는 실제와 예측의 두 확률 분포가 완벽하게 일치하고, $\infty$일때는 정답 분포에서 확률이 1인 클래스에 대해 모델 예측 확률 $Q(x)$가 0에 가까워질 때를 말한다.
- 이는 실제 데이터와 예측데이터가 일치하면 별도의 필요한 정보가 없고, 완전히 다르면 무한대의 정보가 필요함을 의미한다.

---

### Binary Cross Entropy

- 이진 교차 엔트로피는 **이진 분류에 특화된 교차 엔트로피**이다.
- 수식은 아래와 같다.

$$
BCE = -\frac{1}{n}\sum \Big( P(x)\log Q(x) + (1-P(x))\log(1-Q(x)) \Big)

$$

- $P(x)$가 1이라면 앞 부분만 계산된다. 그리고 이때, $Q(x)$가 1이라면 즉, 정답과 일치하면 엔트로피가 0이 된다. 반대로 $Q(x)$가 정답과 다르면(=0) 엔트로피 값은 $\infty$가 된다.
- 이진 교차 엔트로피는 단순히 이진 엔트로피와 교차 엔트로피가 합쳐진거라긴 보다는 **교차 엔트로피의 한 종류로 이진 분류에 적용한 결과**라고 보면 된다.

---

### KL-Divergence

- 쿨백-라이블러 발산은 **두 확률 분포가 얼마나 다른지를 측정**하는 방법으로, **한 확률 분포가 다른 확률 분포와 얼마나 멀리 떨어져 있는지 나타내는 척도**이다.
- 쿨백-라이블러 발산의 수식은 아래와 같다.

$$
D_{KL}(P||Q)=\sum{P(x)log{\frac{P(x)}{Q(x)}}}
$$

- 이 수식을 전개해보면, **교차 엔트로피에서 P에 대한 엔트로피를 빼준 형태**가 된다.

$$
\sum{P(x)log{\frac{P(x)}{Q(x)}}}=-\sum{P(x)log{Q(x)}}-(-\sum{P(x)log{P(x)}})
$$

- 다만, 쿨백-라이블러는 일반적인 분류에서 Loss로서 사용되지는 않는다. 그 이유는 우선 비대칭적이기 때문이다. $D_{KL}(P||Q)$와 $D_{KL}(Q||P)$가 서로 다른 값을 가진다.
- 또한 $[0, \infty]$의 무한한 범위를 다루기에 까다롭기 때문이다. 그러나 정규화 항이나 분포 간 거리를 최소화하는 모델(ex. VAE)에서는 자주 사용된다.

---

### Jensen-Shannon Divergence

- JSD는 KL-Divergence와 같이 두 확률 분포 간의 유사성을 측정하는 방법으로, **KL-Divergence를 두 번 구한 다음 평균을 계산한 개념으로 두 확률분포 간의 거리**를 의미한다.

$$
JSD=\frac{1}{2}D_{KL}(P||M)+\frac{1}{2}D_{KL}(Q||M) \\ M=\frac{1}{2}(P+Q)
$$

- JSD는 KL-Divergence를 양쪽 방향에서 계산한 뒤 평균을 취하는 방식이므로 대칭적이다.
- 또한 JSD는 항상 0과 1 사이의 유한한 값을 가지므로 KL보다 안정적이며, 실전에서 Loss 또는 분포 유사도 측정에 활용할 수 있다.
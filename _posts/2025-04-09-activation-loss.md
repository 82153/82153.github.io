---
layout: article
title: "[DL Basic] 활성화 함수 & 손실 함수"
date: 2025-04-09 12:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---
## 활성화 함수(Activation Function)

- 활성화 함수는 가중 합의 결과를 일정 기준에 따라 값을 변화시키는 **비선형 함수**이다. 이 활성화 함수는 선형으로는 해결할 수 없는 즉, 비선형적으로 분류되는 문제를 해결할 수 있도록 **비선형성을 추가해주는 역할**이다.
- 대표적으로 **Sigmoid, Tanh, ReLU, Softmax** 등이 있다.

### Sigmoid = $\frac 1{1+e^{-x}}$

- 가중 합의 결과를 [0 ~ 1] 사이에서 비선형 형태로 바꿔준다.
    
    ![image.png](attachment:ec086b9a-2913-45b1-90b5-f650bd517974:image.png)
    
- 주로 로지스틱 회귀 같은 분류 문제에서 주로 사용했다.
- 하지만 모델의 깊이가 깊어지며 발생하는 **기울기 소멸 문제가 발생**하여 잘 사용하지 않는다. Sigmoid의 미분 값을 보면 값들이 [0 ~ 0.25] 사이이다. 이는 역전파 시에 기울기가 계속 줄어든다는 의미이고, 계속 곱해지다보면 기울기가 소멸하게 된다.
- 또한 함수의 값이 모두 양수(Non-zero centered)로 학습 시에 **모든 파라미터가 동일한 방향으로 학습하게 되어 불안정한 학습**을 하게 된다.
    
    ![image.png](attachment:94c14800-7844-4f24-aec7-e5b6f0eefb9e:image.png)
    

### **하이퍼볼릭 탄젠트 = Tanh**

- 하이퍼볼릭 탄젠트는 가중 합의 결과를 [-1 ~ 1]사이에서 비선형 형태로 변형해준다.
    
    ![image.png](attachment:fbffd4a2-37cb-45e3-9371-a39c65cab59f:image.png)
    
- **Sigmoid에서 양수만 나오는 문제를 해결**하였다. 하지만 여전히 기울기가 [0 ~ 1]사이로 딥러닝이 깊어지며 발생하는 **기울기 소멸 문제가 발생**한다.

### **ReLU = max(0, x)**

- 최근 가장 활발하게 사용되는 활성화 함수이다. input이 음수이면 0을 출력하고, 양수이면 x를 출력한다. 이는 경사하강법에 영향을 주지 않아 **학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는다.**
    
    ![image.png](attachment:65262df3-42a6-4805-a331-308ac57b22fb:image.png)
    
- 하지만 음수는 항상 0으로 처리하기에 학습 능력이 떨어질 수 있다.

### **Leaky ReLU = x if x > 0 else $\alpha$x**

- ReLU에서 음수를 항상 0을 출력하는 문제를 해결하기 위해 **음수 부분에 매우 작은 기울기**를 주는 방법이다.
    
    ![image.png](attachment:a594d60e-72c6-43d9-8a2d-53698d3f3dae:image.png)
    

### Softmax = $\frac {exp(a)}{\sum_{i=1}^{n}{exp(a)}}$(n = 출력층의 개수)

- 소프트맥스는 입력값을 [0~1] 사이로 출력되도록 정규화하여 출력값들의 값이 항상 1이 된다.
- 주로 출력 노드의 활성화 함수로 많이 사용된다.

---

## 손실 함수

- 손실 함수는 학습을 통해 얻은 데이터의 **추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표**로 **오차를 구하는 방법**이다. 이 값은 0에 가까울 수록 잘 추정한거고, 값이 클수록 많이 틀렸다는 거다.

### 평균 제곱 오차(MSE)

- **실제 값과 예측 값의 차이를 제곱하여 평균** 낸 것이다. 주로 **회귀에서 사용**된다.
    
    ![image.png](attachment:eb7d0a2b-b491-43d1-a529-16c2222175b2:image.png)
    
- 만약 제곱을 하지 않는다면, 차이를 더하는 과정에서 +, -로 인해 차이가 사라지게 되고, 제곱을 하므로써, 오차가 큰 거에 대해 더 큰 페널티를 줄 수 있다.

### 바이너리 크로스 엔트로피(Binary CrossEntropy)

- 바이너리 크로스 엔트로피는 **이진 분류**에서 사용하고, **레이블이 0과 1**로만 나온다.
    
    ![image.png](attachment:01ff2b3a-2008-4fbd-b34c-eb0b7776f9a8:image.png)
    

### 크로스 엔트로피(CrossEntropy)

- 크로스 엔트로피는 **분류 문제**에서 사용되고, **One-Hot Encoding**을 했을 때, 사용할 수 있다.
    
    ![image.png](attachment:af465bcb-a5f6-430b-b0d2-8d313344c802:image.png)
    
- 이진 분류때처럼 1, 2, 3, .. 등으로 나타내지 않는 이유는 Class간에 관계가 생기기 때문이다. 즉, **오차가 Class마다 다르게 된다.**
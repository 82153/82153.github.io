---
layout: article
title: "[DL Basic] 가중치 초기화"
date: 2025-04-14 12:54 +0900
category: DL Basic
tags: [DL Basic]
sidebar:
  nav: dl_basic
---

## 가중치 초기화

- 모델의 최적해에 빠르게 다가가기 위해서는 최적해 근처에서 출발하면 된다. 하지만 우리는 모델의 최적화가 어디에 있는지 알 수 없다. 그렇다면 어느 부분에서 시작을 해야 할까?
- 신경망 학습을 시작할 때, 손실 함수에서 시작 위치를 정하는 것이 **가중치 초기화**이다. 가중치는 모델 파라미터에서 가장 큰 비중을 차지하기에 초기화 방법에 따라 성능이 크게 달라질 수 있다.

### 초기화 방법

1. **상수 초기화(0으로 초기화)**
    - 가장 단순하게 생각해 볼 수 있는 것은 상수로 가중치를 초기화하는 것이다.
    - 만약 모든 가중치를 0으로 초기화하면, 모든 뉴런의 weighted sum이 0이 되고, 이로 인해 활성화 함수의 출력도 항상 동일해진다(Sigmoid는 항상 0.5, Tanh은 항상 0, Softmax는 모든 Class의 확률이 동일한 균등 분포)
    - 또한 모든 가중치가 0이면 학습도 진행되지 않는다.
    - **즉, 0으로 초기화 하면 의미 없는 출력이 만들어지고, 학습이 진행되지 않는다.**
2. **상수 초기화(0이 아닌 상수)**
    - 0이 아닌 상수로 가중치를 초기화 했을 경우, 같은 계층에서 모든 뉴런들의 값이 동일해지는 문제가 발생하게 된다. 이를 **신경망의 대칭성**이라 한다.
    - 이는 여러 뉴런들이 있지만, 하나의 뉴런만 있는 것처럼 작동하게 되어, 정보가 줄어들고 그만큼 성능이 감소하게 된다.
3. **가우시안 분포 초기화**
    - 위에서 발생한 대칭성 문제를 해결하기 위해서는 랜덤한 값들로 가중치를 초기화 해야 한다. 그렇기에 보통 **균등 분포나 가우시안 분포를 따르는 난수를 이용**한다. 아래 예시에서는 가우시안 분포를 따르는 난수에 대하여 볼 것이다.
    - 먼저 모델의 가중치를 $N(0, 0.01)$을 따르는 작은 난수로 초기화 해보자.
    - 평균이 0이고, 분산이 0.01로 매우 작은 값들로 초기화 될 것이다.
    
    ![스크린샷 2025-04-15 160636](https://github.com/user-attachments/assets/31f68278-b4d0-4ee3-94bb-b1502d8cc7a2)

    - 위의 그림은 10개의 계층을 지날 때마다 출력의 분포를 나타낸 것이다.
    - 가중치가 작다면, 당연히 weighted sum의 결과도 0에 가까워지게 될 것이다. 따라서 점점 output이 tanh에서는 0으로, sigmoid에서는 0.5, 즉 **Activation Function(0)의 값으로 몰리게 될 것**이다. 이는 상수로 초기화 하였을 때와 다를 것이 없게 된다.
    - 또한 작은 가중치가 층을 거치며 누적으로 곱해지면서, 출력 값이 점점 작아지고 결국 기울기 소실이 발생한다.
    - 그럼 반대로 가중치를 $N(0, 1)$을 따르는 큰 값으로 초기화 해보자.
    
    ![스크린샷 2025-04-15 161424](https://github.com/user-attachments/assets/2b1517bd-acc2-4b16-a2fc-92caa5ac5c58)

    - 위의 그림을 보면 각 활성화 함수의 양 극단으로 output이 치우치는 것을 알 수 있다.
    - 가중치가 크다면, weight sum의 값이 커질 것이다. 이런 큰 값들이 layer를 거칠 수록 점점 더 커질 것이고, 이 때문에 활성화 함수를 취할 때, 활성화 함수의 양 극단으로 치우치게 되고, 기울기 폭발 문제가 발생한다.
    - 가중치는 너무 커도 안 좋고, 너무 작아도 좋지 않다. 그렇다면 우리는 어떤 가중치를 설정해야 할까
    - 위에서는 데이터가 너무 커지거나, 작아져서 문제였다. 그렇다면 데이터의 크기를 작거나 크게 만들지 않게 해야 할 것이다.
    - **즉, 데이터가 계층을 통과하더라도 데이터의 크기를 유지해주는 가중치로 초기화해야 한다.**
4. **Xavier 초기화**
    - Xavier 초기화는 **Sigmoid, Tanh를 사용할 때, 가중치를 초기화하는 방법으로 입력 데이터의 분산이 출력 데이터에서 유지**되도록 가중치를 초기화해주는 방법이다.
    - Xavier 초기화를 사용하기 위해서는 아래의 가정이 필요하다.
        1. 활성 함수를 선형 함수로 가정하고 입력 데이터는 활성 함수의 가운데 부분을 지난다.
        2. 입력 데이터와 가중치는 서로 독립이다.
        3. 입력 데이터의 각 차원 $x_{i}$는 같은 분포이고 서로 독립인 i.i.d(Independent and identically distributed, 독립 항등 분포)를 만족한다.
        4. 가중치의 각 차원 $w_{i}$도 같은 분포이고 서로 독립인 i.i.d를 만족한다.
        5. 각 $x_{i}$와  $w_{i}$는 평균이 0인 분포를 따른다.
    - Xavier 초기화는 **가중치의 분산이 입력 데이터에 개수 n에 반비례하도록 초기화하는 방**식이다.
    - **유도 과정**
        
        ![스크린샷 2025-04-15 163447](https://github.com/user-attachments/assets/8f9e5317-ca0d-4b5e-bdba-031ece80de57)

        - 즉, $Var(y) = nVar(x_{i})Var(w_{i})$이 되고, 우리는 y의 분산과 x의 분산이 같다고 가정을 하면, 소거되니, $Var(w_{i})$는 $\frac1{n}$이 된다.
    - Xavier 초기화로 가중치 초기화 했을 때, 결과를 보면 여러 계층을 통과하더라도 분산이 잘 유지되는 것을 볼 수 있다.
        
        ![스크린샷 2025-04-15 163836](https://github.com/user-attachments/assets/f7dd0209-4690-4ef9-bd51-50d52db53300)

    - 하지만, ReLU를 사용하면 가중치의 분산이 계층을 통과할 수록 점점 감소하여, 0이 된다.
        
        ![스크린샷 2025-04-15 165142](https://github.com/user-attachments/assets/ff58ffd4-ead5-48dc-a6df-7d0889571f32)

    - 이는 Xavier 초기화에서는 활성화 함수를 선형 함수라 가정하였지만, ReLU의 음수 부분은 위 가정을 만족하지 않기 때문이다.
5. **He 초기화**
    - He 초기화는 Xavier 초기화에서 **ReLU 사용 시의 문제점을 개선한 방식**으로 Xavier 초기화와 동일하게 뉴런의 입력 데이터와 출력 데이터의 분산을 같게 만들어 준다.
    - He 초기화에서는 ReLU를 사용하면, 음수 부분이 모두 0이 되므로, 출력의 분산이 절반으로 줄어들게 된다. 때문에 **He 초기화에서는 가중치의 분산을 2배로 키운 $\frac2{n}$을 사용**한다.
    
        ![스크린샷 2025-04-15 165159](https://github.com/user-attachments/assets/80847629-9366-4a6e-8ad1-7a832b928c52)

    - Xavier 초기화를 ReLU와 함께 사용하면 출력이 0 근처로 몰리는 경향이 있지만, He 초기화는 이러한 문제를 완화해 주어 나머지 데이터가 양수 구간에 골고루 퍼져 있는 것을 볼 수 있다.

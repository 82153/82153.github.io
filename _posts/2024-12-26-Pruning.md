---
layout: article
title: "[Lightweighting]Pruning"
date: 2024-12-26 13:46 +0900
category: Lightweighting_Optimization
tags: [Lightweighting & Optimization]
aside:
  toc: true
sidebar:
  nav: lightweighting
---
## Pruning(가지 치기)

- Pruning이란 신경망 모델에서 노드나 연결을 제거하여 **모델의 크기와 계산 비용을 줄이는 기법**이다.

<img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 124112.png"/>

- 전체적인 과정을 보면 우선 특정 알고리즘을 통해 제거할 파라미터를 판별하여 제거한다.
- 파라미터들을 서로 의존적인 경향이 있어 이렇게 제거하게 되면 성능이 많이 떨어지게 된다. 이를 방지하기 위해 Fine-Tunning을 하여 최종 모델을 만들어 평가한다.
- 이러한 과정을 통해 **메모리 사용을 줄이고, 연산 속도를 가속화** 할 수 있다. 또한 성능도 기존 모델의 성능을 최대한 유지하거나 때로는 개선되는 경우도 있다.(일반적으로는 Pruning정도와 성능은 반비례)

## Pruning 분류 체계

- Pruning 방법의 분류는 크게 4가지 관점으로 볼 수 있다.
    - **Structure**: **Pruning 단위**의 granularity에 따른 분류
    - **Scoring**: Pruning할 **파라미터를 선정하는 방법**에 따른 구분
    - **Scheduling**: Pruning 및 Fine-Tunning을 언제, 얼마나 할 지에 따른 구분
    - **Initialization**: 재학습 및 Fine-Tunning 시 파라미터 초기화를 하는 방법에 따른 구분
- **Structure 관점**
    
    <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 130647.png"/>
    
    - Unstructured Pruning은 여러 개의 **개별 파라미터**를 0으로 변경하는 기법으로 구현이 쉽지만, 하드웨어의 도움이 없으면 가속 실현이 어렵다.
    - Structured Pruning은 Unstructured보다 좀 더 큰 단위를 Pruning하는 것으로 **채널**, **Layer**나 **노드**를 없애는 기법으로 가속 실현이 쉽지만 구현이 어렵거나 불가능한 경우도 존재한다..
    - 두 기법의 큰 차이는 **모델 구조의 변경** 유무이다. Unstructured는 특정 파라미터를 0으로 만드는 과정이기에 **모델의 구조 변화가 없지만**, Structured는 Layer나 노드와 같은 특정 구조를 통째로 제거하는 방법으로 **모델 구조의 변경이 있다**.
- **Scoring 관점**
    - Pruning할 대상을 선정하는 방법에 따른 분류로 **중요도를 계산하는 방식**과 이를 통해 얻은 중요도를 바탕으로 **어디에 반영할지**에 따라 나눈다.
    - **중요도를 계산하는 방법**
        1. 파라미터 별로 절댓값을 중요도로 사용(작을수록 Pruning 대상이다.)
        2. 레이어 별로 $$L^p-norm$$을 중요도로 사용(작을 수로 Pruning 대상이다.)
    - **어디에 반영을 할지**
        1. G**lobal Pruning**
            - 중요도를 전체에서 절대 비교하는 것으로 **모델 전체에서 Pruning**하는 기법이다.
            - 전체에서 비교하기에 계산량이 많지만, 중요한 레이어가 보존된다.
        2. **Local Pruning**
            - 중요도를 단위 내에서만 상대 비교하여 **특정 단위 별로 각각을 Pruning**하는 기법이다.
            - 중요한 Layer가 과도하게 Pruning될 수 있지만, 특정 Layer에 Pruning이 편중되지 않는다.
- **Scheduling 관점**
    - Pruning과 Fine-Tunning을 언제 할 지에 구분에 대해서 **One-shot**과 **Recursive**로 나뉜다.
        
        <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 133109.png"/>
        
    1. **One-Shot(single iteration)**
        - Pruning을 한번에 다 진행하고 이후에 Fine-Tunning을 1번만 진행하는 것이다.
        - 성능이 불안정하지만 시간이 절약된다.
    2. **Recursive(multiple iteration)**
        - Pruning을 한번에 많이 하면 성능 손실이 클 수도 있기에 Pruning을 조금씩 여러 번에 나눠서 진행하는 방법이다.
        - 이때 Pruning 할 때마다, Fine-Tunning을 진행한다.
        - 시간은 오래 걸리지만 성능이 안정적이다.
- **Initialization 관점**
    - Fine-Tunning 하기 전에 파라미터를 어떻게 할지에 대한 것의 구분으로 Prunning 후에 살아남은 weight를 그대로 사용하는 **Weight-preserving**과 살아남은 weight를 랜덤 값으로 초기화 후 재학습을 하는 **Weight-reinitializing**이 있다.
    - **Weight-preserving**
        
        <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 133701.png"/>
        
        - Pruning을 진행한 후의 weight를 그대로 Fine-Tunning을 진행하는 기법으로 학습 및 수렴이 빠르지만 성능이 불안정하다.
    - **Weight-reinitializing**
        
        <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 133922.png"/>
        
        - Pruning을 진행한 후의 weight을 랜덤 값으로 초기화 한 후에 재학습을 진행하는 기법으로 성능이 안정적이지만 재학습이 필요하게 된다.
- **Iterative Magnitude Pruning(IMP)**
    
    <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 135111.png"/>
    
    - 가장 기본적인 Pruning방법으로, **Unstructured Pruning**이고, **파라미터별 절대값**을 중요도로 하고 **Global Pruning**을 사용하고, **Recursive 방식**으로 Scheduling하고, **Rewind 방식(랜덤)으로** Initialization을 한다.
 
## Advanced Concept

- **Matrix Sparsity**
    - 행렬의 대부분의 요소가 0인 행렬을 말하고, 전체 중의 0의 비율이 희소성(Sparsity)이다.
    - Unstructured Pruning은 파라미터 값을 0으로 변경하는 방식인데 희소성이 높은 행렬이라면 계산 속도 측면에서 큰 효과를 볼 수 없을 것이다. 또한 0이라는 값을 저장하고 있기에 메모리 측면에서도 절감할 수 없다.
    - **Sparse Matrix Representation**
        - 희소성이 큰 경우 사용하는 기법으로 0으로 표현된 부분을 제외한 나머지를 저장하는 표현 방식이다.
        
        <img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 141411.png"/>
        
        - 이 방식은 희소성이 $$\frac{1}{3}$$ 미만일 때, 메모리적으로 효율적이다.
        - 위와 같은 조건 때문에 중간 정도의 희소성을 가진 경우에는 전용 하드웨어를 사용한다.
- **Sensitivity Analysis**
    - Layer 별로 Local과 비슷한 Pruning을 하되, 각 Layer별로 민감도에 따라 다른 비율로 Pruning을 하는 기법이다.
    - Global Pruning과 유사해 보이지만, Uniform과 Global의 특징을 둘 다 가진다.
    - 주로 네트워크가 복잡하거나 층별 특성이 상이한 모델의 경우 주로 사용한다.
    - 이 방법은 해당 파라미터/레이어를 pruning했을 때의 성능 저하 정도를 측정하는 실험을 통해 측정하는 Empirical하고 Practical한 방법이지만 보통 가장 앞 부분의 layer가 민감하고 뒷 부분의 layer는 덜 민감하다.

## CNN에서의 Pruning

- CNN에서 대부분의 파라미터는 Fully-Connected에 있기에 Pruning이 필요하고, 연산 속도는 CNN 부분이 좌우하기에 CNN 부분 또한 Prunning이 필요하다.
- **Filter Pruning**
    - 중요도가 작은 Filter를 제거하는 식으로 진행되어 Structured한 방법이라 볼 수 있다. 주로 희소성을 중요도로 하여 희소성이 높은 Filter를 우선적으로 제거한다.
    - Sensitivity Analysis로 확인해본 결과 뒤쪽 Layer가 덜 민감하고, 희소한 Filter의 비율이 높은 Layer는 Pruning을 많이 진행하더라도 성능 저하가 덜 했다.

## BERT에서의 Pruning

<img class="image image--md" src="C:\Users\kdw61\OneDrive\사진\스크린샷\스크린샷 2024-12-26 145433.png"/>

- 언어 모델을 보통 앞쪽 Layer가 단어 같은 작은 형태를 학습하고, 뒷 쪽 Layer가 문장 같은 큰 형태를 학습한다. 하지만 Layer 별로 희소성을 보았을 때는 비 일관적이다.
- 위의 그림을 보았을 때, Global Pruning을 하면 앞 쪽 Layer에서 많이 Pruning이 될텐데, 앞쪽은 Input과 맞닿아 있어 성능 저하가 클 것이기에 부적합하다.
- 또한 Layer 별로 역할이 명확할 것이라 예상되기에 Structured Pruning도 위험하다.
- 그러나 대부분의 파라미터가 0에 가깝기에 절댓값 기준 Pruning은 유효할 것이고, 또한 짧은 단어 일수록 벡터가 더 희소하기에, 짧은 단어 위주의 Pruning이 유효할 수 있다.
